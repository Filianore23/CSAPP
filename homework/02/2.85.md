Intel expansion 
s = 1 , k = 15 , a individual integer bit which equals 1 when normalized and 0 denormalized, n = 63 

bias = 2^(14)-1

1. the smallest positive denormalized number 

bit pattern : 0 0000(15) 0 000(62)1 
decimal : 2^(1-bias-63) // notice that for denormalized value M = f = 0.00000(62)1

2. the smallest positive normalized number

E = 1 - bias , e = 1 , M = 1.000(63)

bit pattern : 0 000(14)1 1 000(63) 
decimal : 2^(1-bias);
3. the biggest normalized number 

e = 111(14)0 -> E = e - bias = bias ; M = 1.111(62)1;

bit pattern : 0 011111(13)1 1 1.11(62)1 

decimal = (2 - 1/(2^(63))) * 2^(bias) ; 
